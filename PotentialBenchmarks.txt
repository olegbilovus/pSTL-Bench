=================
GROUPS:
=================

# Benchmark B1
# Nested (parallel) loops in std::for_each (n -> nlog(n) -> n^2)
Why https://stackoverflow.com/a/71812379/7835214
> I prefer TBB for complex and nested parallelism over OpenMP.(OpenMP has a huge over-head for the nested parallelism)

# Benchmark B2
# Different data types in reduce 
Why https://stackoverflow.com/questions/4763455/data-type-size-impact-on-performance
Depending on the size of the datatype we can have more in cache. 

As far as I can see the size of data types is architecture and compiler dependent (https://stackoverflow.com/a/2331846/7835214). The only thing mentioned in the c++ standard is that char has to be 1 byte and the ranges other data types have to support (https://www.open-std.org/JTC1/SC22/WG14/www/docs/n1256.pdf  https://stackoverflow.com/questions/589575/what-does-the-c-standard-state-the-size-of-int-long-type-to-be).

NVC++ data type sizes: https://docs.nvidia.com/hpc-sdk/compilers/hpc-compilers-ref-guide/#data-types-c-cpp



# Benchmark B3
# Force cache misses by 1) having unpredictable branches 2) std::atomic (accessing shared variable) 3) false sharing (cache line invalidating)

1) http://igoro.com/archive/fast-and-slow-if-statements-branch-prediction-in-modern-processors/ Great pipelining picture


2-3) https://www.youtube.com/watch?v=O0HCGOzFLm0&ab_channel=CoffeeBeforeArch

# Benchmark B4 
# CUTOFFS
# since cutoffs are quite low we have to have a complex compare algorithm
# Some functions like merge have cutoffs in stdlibc++ where they use the serial implementation 
* Merge 	https://github.com/gcc-mirror/gcc/blob/master/libstdc%2B%2B-v3/include/pstl/parallel_backend_tbb.h#L1251 (seems to be #define _PSTL_MERGE_CUT_OFF 2000)
* Stable sort 	https://github.com/gcc-mirror/gcc/blob/master/libstdc%2B%2B-v3/include/pstl/parallel_backend_tbb.h#L1118 (seems to be #define _PSTL_STABLE_SORT_CUT_OFF 500)

(Good source for sorted stuff https://github.com/Morwenn/cpp-sort/wiki/Benchmarks)

SET OPERATION
* set_union 	https://github.com/gcc-mirror/gcc/blob/master/libstdc%2B%2B-v3/include/pstl/algorithm_impl.h#L3046 (seems to be constexpr auto __set_algo_cut_off = 1000; )
* set_diff	https://github.com/gcc-mirror/gcc/blob/master/libstdc%2B%2B-v3/include/pstl/algorithm_impl.h#L3202 (seems to be constexpr auto __set_algo_cut_off = 1000; )
* set_inter	https://github.com/gcc-mirror/gcc/blob/master/libstdc%2B%2B-v3/include/pstl/algorithm_impl.h#L3105
* set_sym_diff	https://github.com/gcc-mirror/gcc/blob/master/libstdc%2B%2B-v3/include/pstl/algorithm_impl.h#L3299 


# Benchmark B5
# Interessting approaches to simple problems (it is actually linear solvable but maybe they found a nice parallelism trick)
* gcc implementation of std::find has comment that maybe there is room for improvements https://github.com/gcc-mirror/gcc/blob/master/libstdc%2B%2B-v3/include/pstl/parallel_impl.h#L29

* partition There are a lot of possible implementation to partition a vector as showed here https://www.cs.upc.edu/~lfrias/research/parpar/parpar.pdf.  (https://github.com/gcc-mirror/gcc/blob/master/libstdc%2B%2B-v3/include/pstl/algorithm_impl.h#L1879)

* unique_copy in gcc seems to use a fancy use of a strict scan https://github.com/gcc-mirror/gcc/blob/master/libstdc%2B%2B-v3/include/pstl/algorithm_impl.h#L1350

# Benchmark B6
# Inclusive_scan/exclusive . Because it's such an important alogirhtm that is building block for so many parallel Algos. https://escholarship.org/content/qt6j57h5zw/qt6j57h5zw.pdf 
# what we try to find out, at what size do they start using simd (only for c20 because unseq is c20) (https://en.algorithmica.org/hpc/algorithms/prefix/), par, etc.  

# Benchmark B7 
# Copy_if logic with std::foreach and once with actual std::copy_if
The goal is to see is there a difference between writing the operation and managing the values by yourself or not. 
Seems there are differences
PS run it once without reserving and once with reserving

# Benchmark B8
# just call std::reduce with ints and transform_reduce large structs(512 bytes maybe).   Because in reduce the chunking and the scheduling is totally work of the library and not of the standard.
Can we draw conclusions from the benchmarks of the chunking etc?
> https://github.com/oneapi-src/oneTBB/blob/0cf592bd62fd4b9673b0f7ea9964c4ec9f80b994/include/oneapi/tbb/parallel_reduce.h#L562
> https://github.com/gcc-mirror/gcc/blob/master/libstdc%2B%2B-v3/include/pstl/parallel_backend_tbb.h#L116

> We can't control the chunking size etc because this comes from the stl. https://oneapi-src.github.io/oneTBB/main/tbb_userguide/Controlling_Chunking_os.html

# Benchmark B9
# std::minmax_element because there is no straight reference standard implementation. For example gcc (stdlibc++) uses under the hood (tbb) reduce.
> https://github.com/gcc-mirror/gcc/blob/master/libstdc%2B%2B-v3/include/pstl/algorithm_impl.h#L3497

# Benchmark B10
# Does closure type of lambdas change performance when using parallel algos? Is there something we can improve?? The standard allows us to do stuff as long as it does not change the intention. 
https://stackoverflow.com/a/32898379/7835214
https://github.com/cplusplus/draft/blob/main/papers/n4431.pdf

# Benchmark B11
# What algos make use of simd efficiently? Reduce (with no params, with SAXPY logic), fill etc

# Benchmark B12
# There is a huge difference between using std::ranges::views::iota and actual container when passing to std::count_if. I implemented B3_6 and B3_5 first with iota view and it did not use any parallel stuff. 

```
const auto &view = std::views::iota(0, static_cast<int>(input_data.size()));

    // we use the range because we do not want to move the input data array around.
    // this call should result in two threads not touching the same cache lines (scheduling should not make this worse)
    return std::count_if(policy, view.begin(), view.end(), [&](const auto &index) {
        return input_data[index].number + input_data[index].second_field >= 0;
    });
```

Then I changed to 

```
return std::count_if(policy, input_data.begin(), input_data.end(), [&](const auto &index) {
        return index.number + index.second_field >= 0;
    });
```

And suddenly parallel concepts where used.